{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PyTorch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import os\n",
    "from torchvision import datasets, transforms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "seed = 1\n",
    "\n",
    "lr = 0.001\n",
    "momentum = 0.5\n",
    "\n",
    "batch_size = 64\n",
    "test_batch_size = 64\n",
    "\n",
    "epochs = 5\n",
    "\n",
    "no_cuda = False\n",
    "log_interval = 100"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Net(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Net, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(3, 20, 5, 1)\n",
    "        self.conv2 = nn.Conv2d(20, 50, 5, 1)\n",
    "        self.fc1 = nn.Linear(4*4*50, 500)\n",
    "        self.fc2 = nn.Linear(500, 10)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.conv1(x))\n",
    "        x = F.max_pool2d(x, 2, 2)\n",
    "        x = F.relu(self.conv2(x))\n",
    "        x = F.max_pool2d(x, 2, 2)\n",
    "        x = x.view(-1, 4*4*50)\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = self.fc2(x)\n",
    "        return F.log_softmax(x, dim=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocess"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dir = 'C:/Users/신상훈/Desktop/새 폴더/dataset/mnist_png/training/'\n",
    "test_dir = 'C:/Users/신상훈/Desktop/새 폴더/dataset/mnist_png/testing/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.manual_seed(seed)\n",
    "\n",
    "use_cuda = not no_cuda and torch.cuda.is_available()\n",
    "device = torch.device(\"cuda\" if use_cuda else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = datasets.ImageFolder(root = train_dir,\n",
    "                                    transform = transforms.Compose([\n",
    "                                        transforms.ToTensor(),\n",
    "                                        transforms.Normalize((0.1307,), (0.3081,))\n",
    "                                    ]))\n",
    "test_dataset = datasets.ImageFolder(root=test_dir,\n",
    "                           transform=transforms.Compose([\n",
    "                               transforms.ToTensor(),\n",
    "                               transforms.Normalize((0.1307,), (0.3081,))\n",
    "                           ]))\n",
    "train_loader = torch.utils.data.DataLoader(train_dataset,\n",
    "                                         batch_size=2,\n",
    "                                         shuffle=True)\n",
    "test_loader = torch.utils.data.DataLoader(test_dataset,\n",
    "                                         batch_size=2\n",
    "                                         )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Optimization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Net().to(device)\n",
    "optimizer = optim.SGD(model.parameters(), lr=lr, momentum=momentum)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 1 [0/60000 (0%)]\tLoss: 2.264696\n",
      "Train Epoch: 1 [200/60000 (0%)]\tLoss: 2.237278\n",
      "Train Epoch: 1 [400/60000 (1%)]\tLoss: 2.195709\n",
      "Train Epoch: 1 [600/60000 (1%)]\tLoss: 2.057271\n",
      "Train Epoch: 1 [800/60000 (1%)]\tLoss: 1.349012\n",
      "Train Epoch: 1 [1000/60000 (2%)]\tLoss: 1.804608\n",
      "Train Epoch: 1 [1200/60000 (2%)]\tLoss: 0.867617\n",
      "Train Epoch: 1 [1400/60000 (2%)]\tLoss: 0.233244\n",
      "Train Epoch: 1 [1600/60000 (3%)]\tLoss: 0.869081\n",
      "Train Epoch: 1 [1800/60000 (3%)]\tLoss: 0.299162\n",
      "Train Epoch: 1 [2000/60000 (3%)]\tLoss: 0.080674\n",
      "Train Epoch: 1 [2200/60000 (4%)]\tLoss: 0.093477\n",
      "Train Epoch: 1 [2400/60000 (4%)]\tLoss: 0.130369\n",
      "Train Epoch: 1 [2600/60000 (4%)]\tLoss: 0.771551\n",
      "Train Epoch: 1 [2800/60000 (5%)]\tLoss: 0.345080\n",
      "Train Epoch: 1 [3000/60000 (5%)]\tLoss: 0.139626\n",
      "Train Epoch: 1 [3200/60000 (5%)]\tLoss: 3.310799\n",
      "Train Epoch: 1 [3400/60000 (6%)]\tLoss: 1.596446\n",
      "Train Epoch: 1 [3600/60000 (6%)]\tLoss: 0.233228\n",
      "Train Epoch: 1 [3800/60000 (6%)]\tLoss: 0.005435\n",
      "Train Epoch: 1 [4000/60000 (7%)]\tLoss: 0.476647\n",
      "Train Epoch: 1 [4200/60000 (7%)]\tLoss: 0.851398\n",
      "Train Epoch: 1 [4400/60000 (7%)]\tLoss: 0.660267\n",
      "Train Epoch: 1 [4600/60000 (8%)]\tLoss: 0.005130\n",
      "Train Epoch: 1 [4800/60000 (8%)]\tLoss: 0.145931\n",
      "Train Epoch: 1 [5000/60000 (8%)]\tLoss: 0.048748\n",
      "Train Epoch: 1 [5200/60000 (9%)]\tLoss: 0.561838\n",
      "Train Epoch: 1 [5400/60000 (9%)]\tLoss: 0.005063\n",
      "Train Epoch: 1 [5600/60000 (9%)]\tLoss: 0.572749\n",
      "Train Epoch: 1 [5800/60000 (10%)]\tLoss: 0.088318\n",
      "Train Epoch: 1 [6000/60000 (10%)]\tLoss: 0.006897\n",
      "Train Epoch: 1 [6200/60000 (10%)]\tLoss: 1.591905\n",
      "Train Epoch: 1 [6400/60000 (11%)]\tLoss: 2.537303\n",
      "Train Epoch: 1 [6600/60000 (11%)]\tLoss: 0.011548\n",
      "Train Epoch: 1 [6800/60000 (11%)]\tLoss: 0.028642\n",
      "Train Epoch: 1 [7000/60000 (12%)]\tLoss: 0.933850\n",
      "Train Epoch: 1 [7200/60000 (12%)]\tLoss: 0.008761\n",
      "Train Epoch: 1 [7400/60000 (12%)]\tLoss: 0.178824\n",
      "Train Epoch: 1 [7600/60000 (13%)]\tLoss: 0.061388\n",
      "Train Epoch: 1 [7800/60000 (13%)]\tLoss: 0.020083\n",
      "Train Epoch: 1 [8000/60000 (13%)]\tLoss: 0.000639\n",
      "Train Epoch: 1 [8200/60000 (14%)]\tLoss: 0.026619\n",
      "Train Epoch: 1 [8400/60000 (14%)]\tLoss: 0.009542\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-14-6ad95f9c7db1>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      8\u001b[0m         \u001b[0moutput\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      9\u001b[0m         \u001b[0mloss\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mF\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnll_loss\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0moutput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtarget\u001b[0m\u001b[1;33m)\u001b[0m  \u001b[1;31m# https://pytorch.org/docs/stable/nn.html#nll-loss\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 10\u001b[1;33m         \u001b[0mloss\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m  \u001b[1;31m# 계산한 기울기를\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     11\u001b[0m         \u001b[0moptimizer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     12\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\torch\\tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[1;34m(self, gradient, retain_graph, create_graph)\u001b[0m\n\u001b[0;32m    219\u001b[0m                 \u001b[0mretain_graph\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mretain_graph\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    220\u001b[0m                 create_graph=create_graph)\n\u001b[1;32m--> 221\u001b[1;33m         \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mautograd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    222\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    223\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mregister_hook\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mhook\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\torch\\autograd\\__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[1;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables)\u001b[0m\n\u001b[0;32m    128\u001b[0m         \u001b[0mretain_graph\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    129\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 130\u001b[1;33m     Variable._execution_engine.run_backward(\n\u001b[0m\u001b[0;32m    131\u001b[0m         \u001b[0mtensors\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mgrad_tensors_\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    132\u001b[0m         allow_unreachable=True)  # allow_unreachable flag\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "for epoch in range(1, epochs + 1):\n",
    "    # Train Mode\n",
    "    model.train()\n",
    "\n",
    "    for batch_idx, (data, target) in enumerate(train_loader):\n",
    "        data, target = data.to(device), target.to(device)\n",
    "        optimizer.zero_grad()  # backpropagation 계산하기 전에 0으로 기울기 계산\n",
    "        output = model(data)\n",
    "        loss = F.nll_loss(output, target)  # https://pytorch.org/docs/stable/nn.html#nll-loss\n",
    "        loss.backward()  # 계산한 기울기를 \n",
    "        optimizer.step()  \n",
    "\n",
    "        if batch_idx % log_interval == 0:\n",
    "            print('Train Epoch: {} [{}/{} ({:.0f}%)]\\tLoss: {:.6f}'.format(\n",
    "                epoch, batch_idx * len(data), len(train_loader.dataset),\n",
    "                100. * batch_idx / len(train_loader), loss.item()))\n",
    "    \n",
    "    # Test mode\n",
    "    model.eval()  # batch norm이나 dropout 등을 train mode 변환\n",
    "    test_loss = 0\n",
    "    correct = 0\n",
    "    with torch.no_grad():  # autograd engine, 즉 backpropagatin이나 gradient 계산 등을 꺼서 memory usage를 줄이고 속도를 높임\n",
    "        for data, target in test_loader:\n",
    "            data, target = data.to(device), target.to(device)\n",
    "            output = model(data)\n",
    "            test_loss += F.nll_loss(output, target, reduction='sum').item() # sum up batch loss\n",
    "            pred = output.argmax(dim=1, keepdim=True) # get the index of the max log-probability\n",
    "            correct += pred.eq(target.view_as(pred)).sum().item()  # pred와 target과 같은지 확인\n",
    "\n",
    "    test_loss /= len(test_loader.dataset)\n",
    "\n",
    "    print('\\nTest set: Average loss: {:.4f}, Accuracy: {}/{} ({:.0f}%)\\n'.format(\n",
    "        test_loss, correct, len(test_loader.dataset),\n",
    "        100. * correct / len(test_loader.dataset)))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
